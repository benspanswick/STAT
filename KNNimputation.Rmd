---
title: "imputation2"
author: "Ben Spanswick"
date: "October 7, 2018"
output: html_document
---

```{r}
rm(list = ls())


#Load Libraries
x = c("ggplot2","DMwR", "outliers","corrgram","rpart","randomForest",
      "corrplot","dplyr","DT","caret","Matrix","data.table","Metrics",
      "ggthemes","clusterSim","usdm")

#Install.packages(x)
#lapply(x, require, character.only = TRUE)

#Remove x
rm(x)

#Read Data
train = read.csv("train.csv",header = T)
View (train)
testfinal= read.csv("test.csv", header = T)
View(test)
#macro = read.csv("macro.csv", header = T)
#View(macro)

#Look at the block of data
head(train, 10)
tail(train, 10)

#Explore the data
str(train)
dim(train)
names(train)

#Missing Values
missing_val = data.frame(apply(train,2,function(x){sum(is.na(x))}))
missing_val$columns = row.names(missing_val)
View(missing_val)
names(missing_val)[1] = "Missing_percentage"
missing_val$Missing_percentage = ( missing_val$Missing_percentage/nrow(train)) * 100
missing_val = missing_val[order(-missing_val$Missing_percentage),]
row.names(missing_val) = NULL
missing_val = missing_val[,c(2,1)]

#Visualization
ggplot(data = missing_val[1:51,], aes(x=reorder(columns,-Missing_percentage),
      y = Missing_percentage)) + geom_bar(stat = "identity", fill = "grey") + 
     xlab("Parameter") + ggtitle("Missing_Data_Percentage (Train)") + theme_bw()

#Data Cleaning
train$state[train$state == 33] <- which.max(table(train$state))

#Delete variables with > 40% Missing Values
train$hospital_beds_raion = NULL
train$build_year = NULL
train$state = NULL
train$cafe_sum_500_max_price_avg = NULL
train$cafe_sum_500_min_price_avg = NULL
train$cafe_avg_price_500 = NULL

#Impute missing values
train = knnImputation(train)

#Save imputed data file
write.csv(train, "train_imp.csv", row.names = F)


```

```{r}

#Check multicollinearity : For numerical values
#Select all numeric data
numeric_data = train[,sapply(train,is.numeric)]

numericnames <- colnames(numeric_data)

str(numericnames)

numform <- as.formula(paste("price_doc ~",paste(numericnames[,-1], collapse= "+")))
numform
#Calculate VIF for data
vif(numform)

#Delete columns where VIF is greater than 10
vif_del = c("area_m","green_zone_part","indust_part","preschool_quota","preschool_education_centers_raion"
,"school_quota","school_education_centers_raion","school_education_centers_top_20_raion"
,"hospital_beds_raion","healthcare_centers_raion","university_top_20_raion","sport_objects_raion",
"additional_education_raion","culture_objects_top_25_raion","shopping_centers_raion",
"office_raion","metro_min_avto","metro_km_avto","park_km","water_treatment_km","cemetery_km",
"incineration_km","railroad_station_avto_min","railroad_station_avto_km","mkad_km","ttk_km",
"sadovoe_km","bulvar_ring_km","kremlin_km","big_road2_km","railroad_km","zd_vokzaly_avto_km",
"oil_chemistry_km","nuclear_reactor_km","radiation_km","power_transmission_line_km",
"thermal_power_plant_km","ts_km","big_market_km","cafe_sum_500_min_price_avg","cafe_sum_500_max_price_avg",
"cafe_avg_price_500","cafe_sum_1000_min_price_avg","cafe_sum_1000_max_price_avg",
"cafe_avg_price_1000","office_count_1500","cafe_sum_1500_min_price_avg","cafe_sum_1500_max_price_avg",
"cafe_avg_price_1500","big_church_count_1500","church_count_1500","leisure_count_1500","sport_count_1500","green_part_2000","prom_part_2000","office_count_2000","office_sqm_2000","trc_count_2000","cafe_sum_2000_min_price_avg","cafe_sum_2000_max_price_avg","cafe_avg_price_2000","big_church_count_2000","leisure_count_2000","sport_count_2000","green_part_3000","prom_part_3000","office_count_3000","office_sqm_3000","trc_count_3000","trc_sqm_3000","cafe_sum_3000_min_price_avg","cafe_sum_3000_max_price_avg","cafe_avg_price_3000","big_church_count_3000","church_count_3000","leisure_count_3000","sport_count_3000","market_count_3000","green_part_5000","prom_part_5000","office_count_5000","office_sqm_5000","trc_sqm_5000","cafe_sum_5000_min_price_avg","cafe_sum_5000_max_price_avg","cafe_sum_5000_max_price_avg","cafe_avg_price_5000","big_church_count_5000","church_count_5000","leisure_count_5000","sport_count_5000",
"market_count_5000")

train = subset(train,select = names(train)[!names(train) %in% vif_del])

```

```{r}
train = read.csv("train_imp.csv",header = T)
```

```{r}

#Chi Square test of Independence : For Factor/Categorical values
factor_data = train[,sapply(train,is.factor)]
for(i in 1:16)
{
  print(names(factor_data)[i])
  print(chisq.test(table(factor_data$product_type,factor_data[,i])))
  print("next variable")
}

```

```{r}
#Outlier detection and removal
df = train
for(i in 1:ncol(df[1:199]))
  { 
  #identifying outliers using boxplot method
  val = df[,i][df[,i] %in% boxplot.stats(df[,i])$out] 
  #df = df[which(!df[,i] %in% val),] 
  df[,i][df[,i] %in% val] = NA #Replace outliers with NA 
}
```

```{r}

train = df[complete.cases(df),]

#Divide the train into train and test
train.index = createDataPartition(train$price_doc, p = 0.80,list = FALSE)
train = train[train.index,]
test = train[-train.index,]

#Random Forest for Regression
#Here, as Random Forest does not accept categorical variables with 
#greater than 53 levels , so we deleted sub_area and timestamp 
train$sub_area = NULL
train$timestamp = NULL
test$sub_area = NULL
test$timestamp = NULL
fit_regrex = randomForest(price_doc ~ ., train, ntree = 500, importance = TRUE)
pred = predict(fit_regrex, test[,])
importance(fit_regrex, type = 1)


```

```{r}
#OLS
model.data <- data.frame(train)
model.data <- model.data[ , -which(names(model.data) %in% c("timestamp"))]

olslm <- lm(price_doc~., data = model.data)
coef(olslm)


```

```{r}

library(glmnet)

x <- model.matrix(price_doc~., model.data)[,-1]
y <- model.data$price_doc
logy <- log(y)
lambda <- 10^seq(10, -2, length = 100)

fit.lasso <-glmnet(x, y, alpha = 1, family = "gaussian")
summary(fit.lasso)
plot(fit.lasso)

fit.lasso.cv <- cv.glmnet(x, y, type.measure="mae", alpha=1)
plot(fit.lasso.cv)

#min value of lambda
lambda_min <- fit.lasso.cv$lambda.min
#best value of lambda
lambda_1se <- fit.lasso.cv$lambda.1se

#regression coefficients
coef(fit.lasso.cv,s=lambda_1se)
coef(fit.lasso.cv, s=lambda_min)

rsq = 1 - fit.lasso.cv$cvm/var(y)
plot(fit.lasso.cv$lambda,rsq)

```

```{r}

fit.elnet <-glmnet(x, logy, alpha = 0.5, family = "gaussian")
summary(fit.elnet)
plot(fit.elnet)

fit.elnet.cv <- cv.glmnet(x, logy, alpha=0.5)
plot(fit.elnet.cv)

#min value of lambda
lambda_min <- fit.elnet.cv$lambda.min
#best value of lambda
lambda_1se <- fit.elnet.cv$lambda.1se

#regression coefficients
coef(fit.elnet.cv,s=lambda_1se)
coef(fit.elnet.cv, s=lambda_min)


```

```{r}
#calculate MAPE
mape <- function(y, yhat)
  mean(abs((y - yhat)/y))
mape(test, pred)
```

```{r}
#Kaggle Test Data
#PreProcess Kaggle Test Data
#Delete variables with > 40% Missing Values
testfinal$hospital_beds_raion = NULL
testfinal$build_year = NULL
testfinal$state = NULL
testfinal$cafe_sum_500_max_price_avg = NULL
testfinal$cafe_sum_500_min_price_avg = NULL
testfinal$cafe_avg_price_500 = NULL

#Dimensionality Reduction using VIF on train dataset
testfinal = subset(testfinal,select = names(testfinal)[!names(testfinal) %in% vif_del])

#Impute missing values
testfinal = knnImputation(testfinal)

#Save imputed data file
write.csv(train, "testfinal_imp.csv", row.names = F)
```

```{r}
# predictions
testfinal$sub_area = NULL
testfinal$timestamp = NULL
fit_regrex = randomForest(price_doc ~ ., train, ntree = 500, importance = TRUE)
pred = predict(fit_regrex,testfinal)
data = data.frame(id=testfinal$id,price_doc=pred)

#Save as csv file for Submission
write.csv(data,file="SubmissionKaggle.csv",row.names=FALSE)
```

```{r}

```

